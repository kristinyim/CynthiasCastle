{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw6_kimy6.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristinyim/CynthiasCastle/blob/master/hw6_kimy6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3FzLpjuaXRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Kristin Yim (kyim6)\n",
        "# Deep Learning Fall 2019\n",
        "# HW 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMyZIU-li7bW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unet.py\n",
        "# This code is provided for Deep Learning (CS 482/682) Homework 6 practice.\n",
        "# The network structure is a simplified U-net. You need to finish the last layers\n",
        "# @Copyright Cong Gao, the Johns Hopkins University, cgao11@jhu.edu\n",
        "# Modified by Hongtao Wu on Oct 11, 2019 for Fall 2019 Machine Learning: Deep Learning HW6\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31KViffojR1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions for adding the convolution layer\n",
        "def add_conv_stage(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=True, useBN=False):\n",
        "  if useBN:\n",
        "    # Use batch normalization\n",
        "    return nn.Sequential(\n",
        "      nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "      nn.BatchNorm2d(dim_out),\n",
        "      nn.LeakyReLU(0.1),\n",
        "      nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "      nn.BatchNorm2d(dim_out),\n",
        "      nn.LeakyReLU(0.1)\n",
        "    )\n",
        "  else:\n",
        "    # No batch normalization\n",
        "    return nn.Sequential(\n",
        "      nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "      nn.ReLU()\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwY6QnjejUSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upsampling\n",
        "def upsample(ch_coarse, ch_fine):\n",
        "  return nn.Sequential(\n",
        "    nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False),\n",
        "    nn.ReLU()\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ss-P8oFkM4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u44-DOaOjWap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# U-Net\n",
        "class unet(nn.Module):\n",
        "  def __init__(self, useBN=False):\n",
        "    super(unet, self).__init__()\n",
        "    # Downgrade stages\n",
        "    self.conv1 = add_conv_stage(3, 32, useBN=useBN)\n",
        "    self.conv2 = add_conv_stage(32, 64, useBN=useBN)\n",
        "    self.conv3 = add_conv_stage(64, 128, useBN=useBN)\n",
        "    self.conv4 = add_conv_stage(128, 256, useBN=useBN)\n",
        "    # Upgrade stages\n",
        "    self.conv3m = add_conv_stage(256, 128, useBN=useBN)\n",
        "    self.conv2m = add_conv_stage(128,  64, useBN=useBN)\n",
        "    self.conv1m = add_conv_stage( 64,  32, useBN=useBN)\n",
        "    # Maxpool\n",
        "    self.max_pool = nn.MaxPool2d(2)\n",
        "    # Upsample layers\n",
        "    self.upsample43 = upsample(256, 128)\n",
        "    self.upsample32 = upsample(128,  64)\n",
        "    self.upsample21 = upsample(64 ,  32)\n",
        "    # weight initialization\n",
        "    # You can have your own weight intialization. This is just an example.\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        if m.bias is not None:\n",
        "          m.bias.data.zero_()\n",
        "\n",
        "    #TODO: Design your last layer & activations\n",
        "    self.conv_last = add_conv_stage(32, 3, useBN=useBN)\n",
        "    self.conv_class = add_conv_stage(3, num_classes, useBN=useBN)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    conv1_out = self.conv1(x) # 3--> 32\n",
        "    conv2_out = self.conv2(self.max_pool(conv1_out)) # 32 --> 64\n",
        "    conv3_out = self.conv3(self.max_pool(conv2_out)) # 64 --> 128\n",
        "    conv4_out = self.conv4(self.max_pool(conv3_out)) # 128 --> 256\n",
        "    # upsample then conv\n",
        "    conv4m_out_ = torch.cat((self.upsample43(conv4_out), conv3_out), 1)\n",
        "    conv3m_out  = self.conv3m(conv4m_out_) # 256 --> 128\n",
        "\n",
        "    conv3m_out_ = torch.cat((self.upsample32(conv3m_out), conv2_out), 1)\n",
        "    conv2m_out  = self.conv2m(conv3m_out_) # 128 --> 64\n",
        "\n",
        "    conv2m_out_ = torch.cat((self.upsample21(conv2m_out), conv1_out), 1)\n",
        "    conv1m_out  = self.conv1m(conv2m_out_) # 64 --> 32\n",
        "\n",
        "    #TODO: Design your last layer & activations\n",
        "    conv_last_out = self.conv_last(conv1m_out) # 32 --> 3\n",
        "    conv_class_out = self.conv_class(conv_last_out) # 3 --> num_classes\n",
        "    out = self.sigmoid(conv_class_out)\n",
        "\n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG8VBtUMjbor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train.py\n",
        "# This code is provided for Deep Learning class (CS 482/682) Homework 6 practice.\n",
        "# This is a sketch code for main function. There are some given hyper-parameters insideself.\n",
        "# You need to finish the design and train your network.\n",
        "# @Copyright Cong Gao, the Johns Hopkins University, cgao11@jhu.edu\n",
        "# Modified by Hongtao Wu on Oct 11, 2019 for Fall 2019 Machine Learning: Deep Learning HW6\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0799clRjloY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################## Hyperparameters #################################\n",
        "# Batch size can be changed if it does not match your memory, please state your batch step_size\n",
        "# in your report.\n",
        "train_batch_size = 10\n",
        "validation_batch_size=10\n",
        "# Please use this learning rate for Prob1(a) and Prob1(b)\n",
        "learning_rate = 0.001\n",
        "# This num_epochs is designed for running to be long enough, you need to manually stop or design\n",
        "# your early stopping method.\n",
        "num_epochs = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W4SP3sTaRcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip /content/drive/My\\ Drive/Colab\\ Notebooks/dl_hw/dl_hw6/segmentation.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnpt7t2SjnwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Design your own dataset\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self,input_dir,transform=None):\n",
        "    self.input_dir = input_dir\n",
        "    inputs = []\n",
        "    masks = []\n",
        "    for root, _, files in os.walk(input_dir):\n",
        "      for file in files:\n",
        "        path = os.path.join(root,file)\n",
        "        if \"input\" in path:\n",
        "          inputs.append(path)\n",
        "        elif \"mask\" in path:\n",
        "          masks.append(path)\n",
        "    self.inputs_path = inputs\n",
        "    self.masks_path = masks\n",
        "    self.transform = transform\n",
        "    self.labels = [0,32,64,96,128,160,192,224]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.inputs_path)\n",
        "  \n",
        "  def load_image(self,file_path):\n",
        "    img = Image.open(file_path)\n",
        "    img.load()\n",
        "    data = np.asarray(img, dtype=\"float32\")\n",
        "    return data\n",
        "  \n",
        "#   def one_hot_y(self,y):\n",
        "#     hot_y = np.zeros((8,y.shape[0],y.shape[1]))\n",
        "#     for i,label in enumerate(self.labels):\n",
        "#       hot_y[i] = np.ma.masked_equal(y,label).mask.astype(int)\n",
        "#     return hot_y\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    # get numpy array\n",
        "    image = self.load_image(self.inputs_path[idx])\n",
        "    mask = self.load_image(self.masks_path[idx])\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    # convert y to one hot encodded with 8 channels  \n",
        "#     mask = self.one_hot_y(mask)\n",
        "    # to torch; image becomes torch in transform already\n",
        "#     mask = torch.from_numpy(mask)\n",
        "      \n",
        "    sample = (image, mask)\n",
        "    return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xyy1-Kljp1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Implement DICE loss\n",
        "class DICELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(DICELoss, self).__init__()\n",
        "      self.labels = [0,32,64,96,128,160,192,224]\n",
        "    \n",
        "    def one_hot_y(self,y):\n",
        "      hot_y = np.zeros((8,y.shape[0],y.shape[1],y.shape[2]))\n",
        "      for i,label in enumerate(self.labels):\n",
        "        hot_y[i] = np.ma.masked_equal(y,label).mask.astype(int)\n",
        "      hot_y = torch.from_numpy(hot_y).permute(1,0,2,3)\n",
        "      return hot_y\n",
        "    \n",
        "    def forward(self,y_hat,y):\n",
        "      # covert y into having 8 channels representing the classes\n",
        "      y = self.one_hot_y(y)\n",
        "      # dice calculation\n",
        "      flat_y_hat = torch.flatten(y_hat)\n",
        "      flat_y = torch.flatten(y)\n",
        "      numerator = torch.matmul(y_hat,y)\n",
        "      print(numerator.shape)\n",
        "      return 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJNuvSLLCfAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = unet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbHPjY5hmt3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c594ab90-ece3-4741-a6ed-71d864e6dee2"
      },
      "source": [
        "train_loader2 = torch.utils.data.DataLoader(train_dataset,batch_size=train_dataset.__len__())\n",
        "for (x,y) in train_loader2:\n",
        "  x_train_mean = torch.mean(x,[0,1,2])\n",
        "  x_train_std = torch.std(x,[0,1,2])\n",
        "print(x_train_mean)\n",
        "print(x_train_std)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([122.4148,  76.8673,  86.2640])\n",
            "tensor([49.7471, 42.8070, 47.1072])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgtftS0vx6-v",
        "colab_type": "text"
      },
      "source": [
        "train -----\n",
        "mean = tensor([122.4148,  76.8673,  86.2640])\n",
        "std = tensor([49.7471, 42.8070, 47.1072])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn5Cq9BT3iLY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "24d6c81c-db0d-46ab-ec01-f4a7d780d2f7"
      },
      "source": [
        "val_loader2 = torch.utils.data.DataLoader(validation_dataset,batch_size=validation_dataset.__len__())\n",
        "for (x,y) in val_loader2:\n",
        "  x_val_mean = torch.mean(x,[0,1,2])\n",
        "  x_val_std = torch.std(x,[0,1,2])\n",
        "print(x_val_mean)\n",
        "print(x_val_std)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([122.3141,  77.3933,  87.1767])\n",
            "tensor([49.8322, 43.1028, 47.3641])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj2SS38r3-Ne",
        "colab_type": "text"
      },
      "source": [
        "val ----\n",
        "mean = tensor([122.3141,  77.3933,  87.1767])\n",
        "std = tensor([49.8322, 43.1028, 47.3641])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTSVGknRkLuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use your designed dataset for dataloading\n",
        "test_path = './segmentation/test'\n",
        "train_path = './segmentation/train'\n",
        "validation_path = './segmentation/validation'\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[122.4148,  76.8673,  86.2640],std=[49.7471, 42.8070, 47.1072])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[122.3141, 77.3933, 87.1767],std=[49.8322, 43.1028, 47.3641])\n",
        "])\n",
        "\n",
        "train_dataset=ImageDataset(input_dir = train_path,transform=train_transforms)\n",
        "validation_dataset=ImageDataset(input_dir = validation_path,transform=val_transforms)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=train_batch_size)\n",
        "val_loader = torch.utils.data.DataLoader(validation_dataset,batch_size=validation_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wwSDJiSRv8K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "cb507d22-ef10-4f76-f0fc-315dc39373f7"
      },
      "source": [
        "sample = train_dataset.__getitem__(0)\n",
        "plt.imshow(sample[0].permute(1,2,0))\n",
        "plt.imshow(sample[1])"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0d923f8cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAD8CAYAAADjcbh8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4CRVy4CTVC0DTlK1DDhJ1TLgJFXr/wF8TCv/tU6TewAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKs33rX_Lis9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step():\n",
        "  model.train()\n",
        "  track_loss = []\n",
        "  for i,(x,y) in enumerate(train_loader):\n",
        "    # maybe needed\n",
        "    if torch.cuda.is_available():\n",
        "      x = x.cuda()\n",
        "      y = y.cuda()\n",
        "      \n",
        "    y_hat = model(x)\n",
        "    # compute loss\n",
        "    loss = loss_func(y_hat,y) # dice\n",
        "    track_loss.append(loss.item())\n",
        "    # update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "  return track_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8RLSbk2FY1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_step():\n",
        "  model.val()\n",
        "  track_loss = []\n",
        "  for i,(x,y) in enumerate(val_loader):\n",
        "    # maybe needed\n",
        "    if torch.cuda.is_available():\n",
        "      x = x.cuda()\n",
        "      y = y.cuda()\n",
        "    \n",
        "    y_hat = model(x)\n",
        "    # compute loss\n",
        "    loss = loss_func(y_hat,y) # dice\n",
        "    track_loss.append(loss.item())\n",
        "\n",
        "  return track_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6qjU7FBkPmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "abf9ca70-d81e-4286-e038-23fa6a45c3e3"
      },
      "source": [
        "print(\"Start Training...\")\n",
        "num_epochs=1\n",
        "train_loss_store = []\n",
        "val_loss_store = []\n",
        "loss_func = DICELoss()\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(num_epochs)+\"\\n\")\n",
        "    # Design your own training section\n",
        "    time_start = time.time()\n",
        "    ########################### Training #####################################\n",
        "    train_loss = train_step()\n",
        "    train_loss_store.extend(train_loss)\n",
        "    print(\"Best training loss: \", min(train_loss_store))\n",
        "    ########################### Validation #####################################\n",
        "    val_loss = val_step()\n",
        "    val_loss_store.extend(val_loss) \n",
        "    print(\"Best validation loss: \", min(val_loss_store))\n",
        "    \n",
        "    time_end = time.time()\n",
        "    print(\"Elapsed time for epoch:\",time_end - time_start,'s')\n",
        "    \n",
        "# plotting\n",
        "steps_store = np.arange(len(train_los_store)).tolist\n",
        "plt.plot(steps_store,train_loss_store,steps_store,val_loss_store)\n",
        "\n",
        "#TODO: testing"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training...\n",
            "\n",
            "EPOCH 1 of 1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk3gj2twWUdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}